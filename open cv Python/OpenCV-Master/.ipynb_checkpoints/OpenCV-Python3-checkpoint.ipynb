{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB Color space based images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = cv2.imread('images/input.jpg')\n",
    "cv2.imshow('Hello OpenCV', input)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(830, 1245, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colorful images have 3 dimensional matrix. so 3d colors.\n",
    "input.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the colorful image's first pixel's R, G, B value ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 18 31\n"
     ]
    }
   ],
   "source": [
    "B, G, R = input[0, 0]\n",
    "print(B, G, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height of an image is 830 pixels\n",
      "weight of an image is 1245 pixels\n"
     ]
    }
   ],
   "source": [
    "print(f'height of an image is {int(input.shape[0])} pixels')\n",
    "print(f'weight of an image is {int(input.shape[1])} pixels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('output.png', input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grayscale Color space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's convert above image to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Grayscale image', gray_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easier and Faster way to convert image to grayscale is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image2 = cv2.imread('images/input.jpg', 0) # 0 means Black & White\n",
    "cv2.imshow('Hello OpenCV', gray_image2)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 1245)\n"
     ]
    }
   ],
   "source": [
    "# gray scale images are made with black and white only, so, they have 2d colors. so matrices will be of 2 dimensional.\n",
    "print(gray_image.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# here, there are no RGB, only one value between 0 to 255 will be there.\n",
    "# this value define the darkness of white to black, that is why it is made of Black & White color.\n",
    "print(gray_image2[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSV ColorSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = cv2.imread('images/input.jpg')\n",
    "\n",
    "hsv_image = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "cv2.imshow('HSV Image', hsv_image)\n",
    "cv2.imshow('Hue channel', hsv_image[:, :, 0])\n",
    "cv2.imshow('Saturation channel', hsv_image[:, :, 1])\n",
    "cv2.imshow('Value channel', hsv_image[:, :, 2])\n",
    "\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# individual representation of colors of RGB Colorspace image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/input.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below, split function split image to color index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 1245) (830, 1245) (830, 1245)\n"
     ]
    }
   ],
   "source": [
    "B, G, R = cv2.split(image)\n",
    "\n",
    "print(R.shape, G.shape, B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now, we'll try to show the red, green and blue part of image individually\n",
    "##### but that will not work as expected, here is the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's show red color part of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Red\", R)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's show green color part of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Green\", G)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's show blue color part of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Blue\", B)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's merge R, G, B matrices to get our image back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B, G, R])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's amplify the blue color intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B+100, G, R])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's amplify the red color intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B, G, R+100])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's amplify the green color intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B, G+100, R])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue + Green = CYAN, let's make CYAN color intensity high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B+100, G+100, R])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "red + Green = yellow, let's make yellow color intensity high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B, G+100, R+100])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "red + blue = purple, let's make purple color intensity high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B+100, G, R+100])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "red + green + blue = whiteness, let's make whiteness intensity high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_image = cv2.merge([B+100, G+100, R+100])\n",
    "cv2.imshow(\"Merged Color based image\", merged_image)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get Red, Blue, and Green colored images individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/input.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get B, G, R colors\n",
    "B, G, R = cv2.split(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we've to pass the R, G and B indicidually to merge function of cv2,\n",
    "# but that will not accept list of one argument, so for other args, we'll pass zero.\n",
    "# we can't use the normal zero for that, so, for that, we'll create numpy zero matrix.\n",
    "\n",
    "zeros = np.zeros(image.shape[:2], dtype = \"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge function take the list in BGR format, so be careful about that\n",
    "cv2.imshow(\"Red\", cv2.merge([zeros, zeros, R]))\n",
    "cv2.imshow(\"Green\", cv2.merge([zeros, G, zeros]))\n",
    "cv2.imshow(\"Blue\", cv2.merge([B, zeros, zeros]))\n",
    "\n",
    "cv2.waitKey(5000) # 5 seconds wait, then destroy windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(830, 1245)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/input.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = cv2.calcHist([image], [0], None, [256], [0, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(image.ravel(), 256, [0, 256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above is only blue color channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's get all three color channel histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ('b', 'g' , 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'color' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-a4c304a64ce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mhistogram2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcHist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistogram2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'color' is not defined"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(color):\n",
    "    histogram2 = cv2.calcHist([image], [i], None, [256], [0, 256])\n",
    "    plt.plot(histogram2, color = col)\n",
    "    plt.xlim([0,256])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating shapes in images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's draw a line in this window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a black image first as a RGB\n",
    "image = np.zeros((512, 512, 3), np.uint8)\n",
    "\n",
    "# line function will take params as : start point(tuple), endpoint(tuple), color RGB(tuple), line width\n",
    "cv2.line(image, (0, 0), (511,511), (255,127,0), 5)\n",
    "cv2.imshow(\"Blue Line\", image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's draw rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a black image first as a RGB\n",
    "image = np.zeros((512, 512, 3), np.uint8)\n",
    "\n",
    "# rectangle function will take params as : start point(tuple), endpoint(tuple), color RGB(tuple), line width\n",
    "cv2.rectangle(image, (100, 100), (300, 250), (127, 50, 127), 5)\n",
    "cv2.imshow(\"Black Rectangle (RGB)\", image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's try to do above things for black and white images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a black image as black and white\n",
    "image_bw = np.zeros((512, 512), np.uint8)\n",
    "\n",
    "# line function will take params as : start point(tuple), endpoint(tuple), color RGB(tuple), line width\n",
    "cv2.line(image_bw, (0, 0), (511,511), (255,127,0), 5)\n",
    "cv2.imshow(\"Black Rectangle (RGB)\", image_bw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a black image as black and white\n",
    "image_bw = np.zeros((512, 512), np.uint8)\n",
    "\n",
    "# rectangle function will take params as : start point(tuple), endpoint(tuple), color RGB(tuple), line width\n",
    "cv2.rectangle(image_bw, (100, 100), (300, 250), (127, 50, 127), 5)\n",
    "cv2.imshow(\"Black Rectangle (RGB)\", image_bw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about the circles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((512, 512, 3), np.uint8)\n",
    "\n",
    "# circle function will take params as :  point to start drawing, radius, color to fill, thickness of line(-1 to fill whole shape)\n",
    "cv2.circle(image, (350, 350), 100, (15, 75, 50), -1)\n",
    "cv2.imshow(\"Circle\", image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 10  50]]\n",
      "\n",
      " [[400  50]]\n",
      "\n",
      " [[ 90 200]]\n",
      "\n",
      " [[ 50 500]]]\n"
     ]
    }
   ],
   "source": [
    "image = np.zeros((512, 512, 3), np.uint8)\n",
    "\n",
    "# let's define points first\n",
    "pts = np.array([[10, 50], [400, 50], [90, 200], [50, 500]], np.int32)\n",
    "\n",
    "# we've to reshape above points matrix to the required form\n",
    "pts = pts.reshape((-1, 1, 2))\n",
    "\n",
    "print(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polylines function take params as : points array, polygon is closed or not (T/F), color, thickness\n",
    "cv2.polylines(image, [pts], True, (0, 0, 255), 3)\n",
    "cv2.imshow(\"Polygon\", image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text in images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's even add text with cv2.putText\n",
    "\n",
    "cv2.putText(image, 'Text to Display', bottom left starting point, Font, Font Size, Color, Thickness)\n",
    "\n",
    "- FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN\n",
    "- FONT_HERSHEY_DUPLEX,FONT_HERSHEY_COMPLEX \n",
    "- FONT_HERSHEY_TRIPLEX, FONT_HERSHEY_COMPLEX_SMALL\n",
    "- FONT_HERSHEY_SCRIPT_SIMPLEX\n",
    "- FONT_HERSHEY_SCRIPT_COMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((512,512,3), np.uint8)\n",
    "\n",
    "cv2.putText(image, 'Hello World!', (75,290), cv2.FONT_HERSHEY_COMPLEX, 2, (100,170,0), 3)\n",
    "cv2.imshow(\"Hello World!\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translations\n",
    "\n",
    "This an affine transform that simply shifts the position of an image.\n",
    "\n",
    "We use cv2.warpAffine to implement these transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/input.jpg')\n",
    "\n",
    "# Store height and width of the image\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "quarter_height, quarter_width = height/4, width/4\n",
    "\n",
    "#       | 1 0 Tx |\n",
    "#  T  = | 0 1 Ty |\n",
    "\n",
    "# T is our translation matrix\n",
    "T = np.float32([[1, 0, quarter_width], [0, 1,quarter_height]])\n",
    "\n",
    "# We use warpAffine to transform the image using the matrix, T\n",
    "img_translation = cv2.warpAffine(image, T, (width, height))\n",
    "cv2.imshow('Translation', img_translation)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.     0.   311.25]\n",
      " [  0.     1.   207.5 ]]\n"
     ]
    }
   ],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotations\n",
    "\n",
    "cv2.getRotationMatrix2D(rotation_center_x, rotation_center_y, angle of rotation, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/input.jpg')\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Divide by two to rototate the image around its centre\n",
    "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, .5)\n",
    "\n",
    "rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "\n",
    "cv2.imshow('Rotated Image', rotated_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice all the black space surrounding the image.\n",
    "\n",
    "We could now crop the image as we can calculate it's new size (we haven't learned cropping yet!).\n",
    "\n",
    "But here's another method for simple rotations that uses the cv2.transpose function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other Option to Rotate\n",
    "img = cv2.imread('images/input.jpg')\n",
    "\n",
    "rotated_image = cv2.transpose(img)\n",
    "\n",
    "cv2.imshow('Rotated Image - Method 2', rotated_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now to a horizontal flip.\n",
    "flipped = cv2.flip(image, 1)\n",
    "cv2.imshow('Horizontal Flip', flipped) \n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling, re-sizing and interpolations\n",
    "\n",
    "Re-sizing is very easy using the cv2.resize function, it's arguments are:\n",
    "\n",
    "cv2.resize(image, dsize(output image size), x scale, y scale, interpolation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# load our input image\n",
    "image = cv2.imread('images/input.jpg')\n",
    "\n",
    "# Let's make our image 1/2 of it's original size, so 0.50 will be scale of x, y\n",
    "image_scaled = cv2.resize(image, None, fx=0.50, fy=0.50)\n",
    "cv2.imshow('Scaling - Linear Interpolation', image_scaled) \n",
    "cv2.waitKey()\n",
    "\n",
    "# Let's double the size of our image\n",
    "img_scaled = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "cv2.imshow('Scaling - Cubic Interpolation', img_scaled)\n",
    "cv2.waitKey()\n",
    "\n",
    "# Let's skew the re-sizing by setting exact dimensions\n",
    "# size of 100 x 400 is set here\n",
    "img_scaled = cv2.resize(image, (100, 400), interpolation = cv2.INTER_AREA)\n",
    "cv2.imshow('Scaling - Skewed Size', img_scaled) \n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pyramids\n",
    "\n",
    "Useful when scaling images in object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/input.jpg')\n",
    "\n",
    "smaller = cv2.pyrDown(image) # mak3 image 50 % smaller\n",
    "larger = cv2.pyrUp(smaller) # mak3 image 50 % bigger\n",
    "\n",
    "cv2.imshow('Original', image )\n",
    "\n",
    "cv2.imshow('Smaller ', smaller )\n",
    "cv2.imshow('Larger ', larger )\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/input.jpg')\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Let's get the starting pixel coordiantes (top  left of cropping rectangle)\n",
    "start_row, start_col = int(height * .25), int(width * .25)\n",
    "\n",
    "# Let's get the ending pixel coordinates (bottom right)\n",
    "end_row, end_col = int(height * .75), int(width * .75)\n",
    "\n",
    "# Simply use indexing to crop out the rectangle we desire\n",
    "cropped = image[start_row:end_row , start_col:end_col]\n",
    "\n",
    "cv2.imshow(\"Original Image\", image)\n",
    "cv2.waitKey(0) \n",
    "cv2.imshow(\"Cropped Image\", cropped) \n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Operations\n",
    "\n",
    "These are simple operations that allow us to directly add or subract to the color intensity.\n",
    "\n",
    "Calculates the per-element operation of two arrays. The overall effect is increasing or decreasing brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/input.jpg')\n",
    "\n",
    "# Create a matrix of ones, then multiply it by a scaler of 100 \n",
    "# This gives a matrix with same dimesions of our image with all values being 100\n",
    "M = np.ones(image.shape, dtype = \"uint8\") * 50 \n",
    "\n",
    "# We use this to add this matrix M, to our image\n",
    "# Notice the increase in brightness\n",
    "added = cv2.add(image, M)\n",
    "cv2.imshow(\"Added\", added)\n",
    "\n",
    "# Likewise we can also subtract\n",
    "# Notice the decrease in brightness\n",
    "subtracted = cv2.subtract(image, M)\n",
    "cv2.imshow(\"Subtracted\", subtracted)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        ...,\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75]],\n",
       "\n",
       "       [[75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        ...,\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75]],\n",
       "\n",
       "       [[75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        ...,\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        ...,\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75]],\n",
       "\n",
       "       [[75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        ...,\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75]],\n",
       "\n",
       "       [[75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        ...,\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75],\n",
       "        [75, 75, 75]]], dtype=uint8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.ones(image.shape, dtype = \"uint8\") * 75 \n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitwise Operations and Masking\n",
    "\n",
    "To demonstrate these operations let's create some simple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# If you're wondering why only two dimensions, well this is a grayscale image, \n",
    "# if we doing a colored image, we'd use \n",
    "# rectangle = np.zeros((300, 300, 3),np.uint8)\n",
    "\n",
    "# Making a sqare\n",
    "square = np.zeros((300, 300), np.uint8)\n",
    "cv2.rectangle(square, (50, 50), (250, 250), 255, -2)\n",
    "cv2.imshow(\"Square\", square)\n",
    "\n",
    "# Making a ellipse\n",
    "ellipse = np.zeros((300, 300), np.uint8)\n",
    "cv2.ellipse(ellipse, (150, 150), (150, 150), 300, 0, 180, 255, -1)\n",
    "cv2.imshow(\"Ellipse\", ellipse)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with some bitwise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows only where they intersect\n",
    "And = cv2.bitwise_and(square, ellipse)\n",
    "cv2.imshow(\"AND\", And)\n",
    "\n",
    "# Shows where either square or ellipse is \n",
    "bitwiseOr = cv2.bitwise_or(square, ellipse)\n",
    "cv2.imshow(\"OR\", bitwiseOr)\n",
    "\n",
    "# Shows where either exist by itself\n",
    "bitwiseXor = cv2.bitwise_xor(square, ellipse)\n",
    "cv2.imshow(\"XOR\", bitwiseXor)\n",
    "\n",
    "# Shows everything that isn't part of the square\n",
    "bitwiseNot_sq = cv2.bitwise_not(square)\n",
    "cv2.imshow(\"NOT - square\", bitwiseNot_sq)\n",
    "\n",
    "# Shows everything that isn't part of the ellipse\n",
    "bitwiseNot_sq2 = cv2.bitwise_not(ellipse)\n",
    "cv2.imshow(\"NOT - ellipse\", bitwiseNot_sq2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "### Notice the last operation inverts the image totally\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions and Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/elephant.jpg')\n",
    "cv2.imshow('Original Image', image)\n",
    "\n",
    "# Creating our 3 x 3 kernel\n",
    "kernel_3x3 = np.ones((3, 3), np.float32) / 9\n",
    "\n",
    "# We use the cv2.fitler2D to conovlve the kernal with an image \n",
    "blurred = cv2.filter2D(image, -1, kernel_3x3)\n",
    "cv2.imshow('3x3 Kernel Blurring', blurred)\n",
    "\n",
    "# Creating our 7 x 7 kernel\n",
    "kernel_7x7 = np.ones((7, 7), np.float32) / 49\n",
    "\n",
    "blurred2 = cv2.filter2D(image, -1, kernel_7x7)\n",
    "cv2.imshow('7x7 Kernel Blurring', blurred2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111]]\n",
      "[[0.02040816 0.02040816 0.02040816 0.02040816 0.02040816 0.02040816\n",
      "  0.02040816]\n",
      " [0.02040816 0.02040816 0.02040816 0.02040816 0.02040816 0.02040816\n",
      "  0.02040816]\n",
      " [0.02040816 0.02040816 0.02040816 0.02040816 0.02040816 0.02040816\n",
      "  0.02040816]\n",
      " [0.02040816 0.02040816 0.02040816 0.02040816 0.02040816 0.02040816\n",
      "  0.02040816]\n",
      " [0.02040816 0.02040816 0.02040816 0.02040816 0.02040816 0.02040816\n",
      "  0.02040816]\n",
      " [0.02040816 0.02040816 0.02040816 0.02040816 0.02040816 0.02040816\n",
      "  0.02040816]\n",
      " [0.02040816 0.02040816 0.02040816 0.02040816 0.02040816 0.02040816\n",
      "  0.02040816]]\n"
     ]
    }
   ],
   "source": [
    "print(np.ones((3, 3), np.float32) / 9)\n",
    "print(np.ones((7, 7), np.float32) / 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other commonly used blurring methods in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/elephant.jpg')\n",
    "\n",
    "# Averaging done by convolving the image with a normalized box filter. \n",
    "# This takes the pixels under the box and replaces the central element\n",
    "# Box size needs to odd and positive \n",
    "blur = cv2.blur(image, (3,3))\n",
    "cv2.imshow('Averaging', blur)\n",
    "\n",
    "# Instead of box filter, gaussian kernel\n",
    "# here, third param is amount of blurness which is zero here.\n",
    "Gaussian = cv2.GaussianBlur(image, (7,7), 0)\n",
    "cv2.imshow('Gaussian Blurring', Gaussian)\n",
    "\n",
    "# Takes median of all the pixels under kernel area and central \n",
    "# element is replaced with this median value\n",
    "median = cv2.medianBlur(image, 5)\n",
    "cv2.imshow('Median Blurring', median)\n",
    "\n",
    "# Bilateral is very effective in noise removal while keeping edges sharp\n",
    "bilateral = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "cv2.imshow('Bilateral Blurring', bilateral)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image De-noising - Non-Local Means Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/elephant.jpg')\n",
    "\n",
    "# Parameters, after None are - the filter strength 'h' (5-10 is a good range)\n",
    "# Next is hForColorComponents, set as same value as h again\n",
    "# \n",
    "dst = cv2.fastNlMeansDenoisingColored(image, None, 6, 6, 7, 21)\n",
    "cv2.imshow(\"Original\", image)\n",
    "cv2.imshow('Fast Means Denoising', dst)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharpening \n",
    "\n",
    "By altering our kernels we can implement sharpening, which has the effects of in strengthening or emphasizing edges in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/input.jpg')\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "# Create our shapening kernel, we don't normalize since the \n",
    "# the values in the matrix sum to 1\n",
    "kernel_sharpening = np.array([[-1,-1,-1], \n",
    "                              [-1,9,-1], \n",
    "                              [-1,-1,-1]])\n",
    "\n",
    "# applying different kernels to the input image\n",
    "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
    "\n",
    "cv2.imshow('Image Sharpening', sharpened)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/elephant.jpg')\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "# Create our shapening kernel, we don't normalize since the \n",
    "# the values in the matrix sum to 1\n",
    "kernel_sharpening = np.array([[-1,-1,-1], \n",
    "                              [-1,9,-1], \n",
    "                              [-1,-1,-1]])\n",
    "\n",
    "# applying different kernels to the input image\n",
    "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
    "\n",
    "cv2.imshow('Image Sharpening', sharpened)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding, Binarization & Adaptive Thresholding\n",
    "#### thresholding convert image into lowest point andhighest point, means 0 and 255,\n",
    "#### means values less than 127 will become 0 and values greater than 127 will become 255 \n",
    "#### thresholding works with only greyscale images\n",
    "#### So, In thresholding, we convert a grey scale image to it's binary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load our image as greyscale \n",
    "image = cv2.imread('images/gradient.jpg',0)\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "# Values below 127 goes to 0 (black, everything above goes to 255 (white)\n",
    "ret,thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('1 Threshold Binary', thresh1)\n",
    "\n",
    "# Values below 127 go to 255 and values above 127 go to 0 (reverse of above)\n",
    "ret,thresh2 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "cv2.imshow('2 Threshold Binary Inverse', thresh2)\n",
    "\n",
    "# Values above 127 will be changed to 127 (the 255 argument is unused)\n",
    "ret,thresh3 = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)\n",
    "cv2.imshow('3 THRESH TRUNC', thresh3)\n",
    "\n",
    "# Values below 127 go to 0, above 127 are unchanged  \n",
    "ret,thresh4 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)\n",
    "cv2.imshow('4 THRESH TOZERO', thresh4)\n",
    "\n",
    "# Resever of above, below 127 is unchanged, above 127 goes to 0\n",
    "ret,thresh5 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "cv2.imshow('5 THRESH TOZERO INV', thresh5)\n",
    "cv2.waitKey(0) \n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a better way of thresholding?\n",
    "\n",
    "The biggest downfall of those simple threshold methods is that we need to provide the threshold value (i.e. the 127 value we used previously).\n",
    "#### What if there was a smarter way of doing this?\n",
    "\n",
    "There is with, Adaptive thresholding. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load our new image\n",
    "image = cv2.imread('images/Origin_of_Species.jpg', 0)\n",
    "\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0) \n",
    "\n",
    "# Values below 127 goes to 0 (black, everything above goes to 255 (white)\n",
    "ret,thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('Threshold Binary', thresh1)\n",
    "cv2.waitKey(0) \n",
    "\n",
    "# It's good practice to blur images as it removes noise\n",
    "# GaussianBlur Params are : destImage, kernel size to blur, Kernel standard deviation along X-axis (horizontal direction).\n",
    "image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "# Using adaptiveThreshold\n",
    "# params : image, max value of color, adaptive type, threshold type, Block size (should be odd), Constant that is subtracted from mean\n",
    "# ADAPTIVE_THRESH_MEAN_C : based on mean of the neighbourhood of pixels\n",
    "# ADAPTIVE_THRESH_GAUSSIAN_C : weighted sum of neighbourhood pixels under the Gaussian window\n",
    "thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                               cv2.THRESH_BINARY, 3, 5) \n",
    "cv2.imshow(\"Adaptive Mean Thresholding\", thresh) \n",
    "cv2.waitKey(0) \n",
    "\n",
    "_, th2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "cv2.imshow(\"Otsu's Thresholding\", thresh) \n",
    "cv2.waitKey(0) \n",
    "\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv2.GaussianBlur(image, (5,5), 0)\n",
    "_, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "cv2.imshow(\"Guassian Otsu's Thresholding\", thresh) \n",
    "cv2.waitKey(0) \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilation, Erosion, Opening and Closing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilation : add pixels at the boundaries of an image\n",
    "#### Erosion : remove pixels at the boundaries of an image\n",
    "#### Opening : Erosion following by Dilation\n",
    "#### Closing : Dilation following by Erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/opencv_inv.png', 0)\n",
    "\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Let's define our kernel size\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "# here, we'll provide iteration parameter, which will define how many time that operation should be applied\n",
    "# Now we erode\n",
    "erosion = cv2.erode(image, kernel, iterations = 1)\n",
    "cv2.imshow('Erosion', erosion)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Now let's dilate\n",
    "dilation = cv2.dilate(image, kernel, iterations = 1)\n",
    "cv2.imshow('Dilation', dilation)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Opening - Good for removing noise\n",
    "opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow('Opening', opening)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Closing - Good for removing noise\n",
    "closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imshow('Closing', closing)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are some other less popular morphology operations, see the official OpenCV site:\n",
    "\n",
    "http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detection & Image Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/input.jpg',0)\n",
    "\n",
    "height, width = image.shape\n",
    "\n",
    "# Extract Sobel Edges\n",
    "# here, we'll extract vertical and horizontal lines\n",
    "sobel_x = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5) # horiz\n",
    "sobel_y = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5) # verti\n",
    "\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('Sobel X', sobel_x)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('Sobel Y', sobel_y)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "sobel_OR = cv2.bitwise_or(sobel_x, sobel_y)\n",
    "cv2.imshow('sobel_OR', sobel_OR)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "cv2.imshow('Laplacian', laplacian)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Then, we need to provide two values: threshold1 and threshold2. Any gradient value larger than threshold2\n",
    "# is considered to be an edge. Any value below threshold1 is considered not to be an edge. \n",
    "#Values in between threshold1 and threshold2 are either classied as edges or non-edges based on how their \n",
    "#intensities are connected. In this case, any gradient values below 60 are considered non-edges\n",
    "#whereas any values above 120 are considered edges.\n",
    "\n",
    "\n",
    "# Canny Edge Detection uses gradient values as thresholds\n",
    "# The first threshold gradient\n",
    "canny = cv2.Canny(image, 50, 120)\n",
    "cv2.imshow('Canny', canny)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting Perpsective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread('images/scan.jpg')\n",
    "\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Cordinates of the 4 corners of the original image\n",
    "points_A = np.float32([[320,15], [700,215], [85,610], [530,780]])\n",
    "\n",
    "# Cordinates of the 4 corners of the desired output\n",
    "# We use a ratio of an A4 Paper 1 : 1.41\n",
    "points_B = np.float32([[0,0], [420,0], [0,594], [420,594]])\n",
    " \n",
    "# Use the two sets of four points to compute \n",
    "# the Perspective Transformation matrix, M    \n",
    "M = cv2.getPerspectiveTransform(points_A, points_B)\n",
    " \n",
    "warped = cv2.warpPerspective(image, M, (420,594))\n",
    " \n",
    "cv2.imshow('warpPerspective', warped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In affine transforms you only need 3 coordiantes to obtain the correct transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread('images/ex2.jpg')\n",
    "rows,cols,ch = image.shape\n",
    "\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Cordinates of the 4 corners of the original image\n",
    "points_A = np.float32([[320,15], [700,215], [85,610]])\n",
    "\n",
    "# Cordinates of the 4 corners of the desired output\n",
    "# We use a ratio of an A4 Paper 1 : 1.41\n",
    "points_B = np.float32([[0,0], [420,0], [0,594]])\n",
    " \n",
    "# Use the two sets of four points to compute \n",
    "# the Perspective Transformation matrix, M    \n",
    "M = cv2.getAffineTransform(points_A, points_B)\n",
    "\n",
    "warped = cv2.warpAffine(image, M, (cols, rows))\n",
    " \n",
    "cv2.imshow('warpPerspective', warped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.91504943    0.36140608 -298.23690956]\n",
      " [  -0.43500549    0.82651044  126.80410106]]\n"
     ]
    }
   ],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Project, live sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch(image):\n",
    "    # greyscale first\n",
    "    image_grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # then clean up by gaussian blur\n",
    "    image_grey_blur = cv2.GaussianBlur(image_grey, (5,5), 0)\n",
    "    \n",
    "    # Extract edges by canny\n",
    "    canny_edges = cv2.Canny(image_grey_blur, 10, 70)\n",
    "    \n",
    "    # invert the binarize the image\n",
    "    ret, mask = cv2.threshold(canny_edges, 70, 255, cv2.THRESH_BINARY_INV)\n",
    "    return mask\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow(\"Live sketch\", sketch(frame))\n",
    "    if cv2.waitKey(1) == 13: # Enter\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contours found = 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Let's load a simple image with 3 black squares\n",
    "image = cv2.imread('images/shapes.jpg')\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Grayscale\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find Canny edges\n",
    "edged = cv2.Canny(gray, 30, 200)\n",
    "cv2.imshow('Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Finding Contours\n",
    "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
    "contours, hierarchy = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "cv2.imshow('Canny Edges After Contouring', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(\"Number of Contours found = \" + str(len(contours)))\n",
    "\n",
    "# Draw all contours\n",
    "# Use '-1' as the 3rd parameter to draw all\n",
    "cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contours found = 8\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Let's load a simple image with 3 black squares\n",
    "image = cv2.imread('images/shapes_donut.jpg')\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Grayscale\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find Canny edges\n",
    "edged = cv2.Canny(gray, 30, 200)\n",
    "cv2.imshow('Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Finding Contours\n",
    "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
    "contours, hierarchy = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "cv2.imshow('Canny Edges After Contouring', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(\"Number of Contours found = \" + str(len(contours)))\n",
    "\n",
    "# Draw all contours\n",
    "# Use '-1' as the 3rd parameter to draw all\n",
    "cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cv2.findContours(image, Retrieval Mode, Approximation Method)**\n",
    "\n",
    "Returns -> contours, hierarchy\n",
    "\n",
    "**NOTE** In OpenCV 3.X, findContours returns a 3rd argument which is ret (or a boolean indicating if the function was successfully run). \n",
    "\n",
    "If you're using OpenCV 3.X replace line 12 with:\n",
    "\n",
    "_, contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "The variable 'contours' are stored as a numpy array of (x,y) points that form the contour\n",
    "\n",
    "While, 'hierarchy' describes the child-parent relationships between contours (i.e. contours within contours)\n",
    "\n",
    "\n",
    "\n",
    "#### Approximation Methods\n",
    "\n",
    "Using cv2.CHAIN_APPROX_NONE stores all the boundary points. But we don't necessarily need all bounding points. If the points form a straight line, we only need the start and ending points of that line.\n",
    "\n",
    "Using cv2.CHAIN_APPROX_SIMPLE instead only provides these start and end points of bounding contours, thus resulting in much more efficent storage of contour information.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Contours\n",
    "\n",
    "We can sort contours in many ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contours found =  4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load our image\n",
    "image = cv2.imread('images/bunchofshapes.jpg')\n",
    "cv2.imshow('0 - Original Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create a black image with same dimensions as our loaded image\n",
    "blank_image = np.zeros((image.shape[0], image.shape[1], 3))\n",
    "\n",
    "# Create a copy of our original image\n",
    "orginal_image = image\n",
    "\n",
    "# Grayscale our image\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find Canny edges\n",
    "edged = cv2.Canny(gray, 50, 200)\n",
    "cv2.imshow('1 - Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Find contours and print how many were found\n",
    "contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "print (\"Number of contours found = \", len(contours))\n",
    "\n",
    "#Draw all contours\n",
    "cv2.drawContours(blank_image, contours, -1, (0,255,0), 3)\n",
    "cv2.imshow('2 - All Contours over blank image', blank_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Draw all contours over blank image\n",
    "cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "cv2.imshow('3 - All Contours', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now sort by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contor Areas before sorting\n",
      "[20587.5, 22901.5, 66579.5, 90222.0]\n",
      "Contor Areas after sorting\n",
      "[90222.0, 66579.5, 22901.5, 20587.5]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function we'll use to display contour area\n",
    "\n",
    "def get_contour_areas(contours):\n",
    "    # returns the areas of all contours as list\n",
    "    all_areas = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        all_areas.append(area)\n",
    "    return all_areas\n",
    "\n",
    "# Load our image\n",
    "image = cv2.imread('images/bunchofshapes.jpg')\n",
    "orginal_image = image\n",
    "\n",
    "# Let's print the areas of the contours before sorting\n",
    "print(\"Contor Areas before sorting\") \n",
    "print(get_contour_areas(contours))\n",
    "\n",
    "# Sort contours large to small\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:3]\n",
    "\n",
    "print (\"Contor Areas after sorting\")\n",
    "print (get_contour_areas(sorted_contours))\n",
    "\n",
    "# Iterate over our contours and draw one at a time\n",
    "for c in sorted_contours:\n",
    "    cv2.drawContours(orginal_image, [c], -1, (255,0,0), 3)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.imshow('Contours by area', orginal_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape_number_1.jpg\n",
      "output_shape_number_2.jpg\n",
      "output_shape_number_3.jpg\n",
      "output_shape_number_4.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Functions we'll use for sorting by position\n",
    "\n",
    "def x_cord_contour(contours):\n",
    "    #Returns the X cordinate for the contour centroid\n",
    "    if cv2.contourArea(contours) > 10:\n",
    "        M = cv2.moments(contours)\n",
    "        return (int(M['m10']/M['m00']))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "def label_contour_center(image, c):\n",
    "    # Places a red circle on the centers of contours\n",
    "    M = cv2.moments(c)\n",
    "    cx = int(M['m10'] / M['m00'])\n",
    "    cy = int(M['m01'] / M['m00'])\n",
    " \n",
    "    # Draw the countour number on the image\n",
    "    cv2.circle(image,(cx,cy), 10, (0,0,255), -1)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Load our image\n",
    "image = cv2.imread('images/bunchofshapes.jpg')\n",
    "orginal_image = image.copy()\n",
    "\n",
    "\n",
    "# Computer Center of Mass or centroids and draw them on our image\n",
    "for (i, c) in enumerate(contours):\n",
    "    orig = label_contour_center(image, c)\n",
    " \n",
    "cv2.imshow(\"4 - Contour Centers \", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Sort by left to right using our x_cord_contour function\n",
    "contours_left_to_right = sorted(contours, key = x_cord_contour, reverse = False)\n",
    "\n",
    "\n",
    "# Labeling Contours left to right\n",
    "for (i,c)  in enumerate(contours_left_to_right):\n",
    "    cv2.drawContours(orginal_image, [c], -1, (0,0,255), 3)  \n",
    "    M = cv2.moments(c)\n",
    "    cx = int(M['m10'] / M['m00'])\n",
    "    cy = int(M['m01'] / M['m00'])\n",
    "    cv2.putText(orginal_image, str(i+1), (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('6 - Left to Right Contour', orginal_image)\n",
    "    cv2.waitKey(0)\n",
    "    (x, y, w, h) = cv2.boundingRect(c)  \n",
    "    \n",
    "    # Let's now crop each contour and save these images\n",
    "    cropped_contour = orginal_image[y:y + h, x:x + w]\n",
    "    image_name = \"output_shape_number_\" + str(i+1) + \".jpg\"\n",
    "    print(image_name)\n",
    "    cv2.imwrite(image_name, cropped_contour)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating Contours and Convex Hull \n",
    "\n",
    "***cv2.approxPolyDP(contour, Approximation Accuracy, Closed)***\n",
    "- **contour**  is the individual contour we wish to approximate\n",
    "- **Approximation Accuracy**  Important parameter is determining the accuracy of the approximation. Small values give precise-  approximations, large values give more generic approximation. A good rule of thumb is less than 5% of the contour perimeter\n",
    "- **Closed**  a Boolean value that states whether the approximate contour should be open or closed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load image and keep a copy\n",
    "image = cv2.imread('images/house.jpg')\n",
    "orig_image = image.copy()\n",
    "cv2.imshow('Original Image', orig_image)\n",
    "cv2.waitKey(0) \n",
    "\n",
    "# Grayscale and binarize\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Find contours \n",
    "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# Iterate through each contour and compute the bounding rectangle\n",
    "for c in contours:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    cv2.rectangle(orig_image,(x,y),(x+w,y+h),(0,0,255),2)    \n",
    "    cv2.imshow('Bounding Rectangle', orig_image)\n",
    "\n",
    "cv2.waitKey(0) \n",
    "    \n",
    "# Iterate through each contour and compute the approx contour\n",
    "for c in contours:\n",
    "    # Calculate accuracy as a percent of the contour perimeter\n",
    "    accuracy = 0.03 * cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, accuracy, True)\n",
    "    cv2.drawContours(image, [approx], 0, (0, 255, 0), 2)\n",
    "    cv2.imshow('Approx Poly DP', image)\n",
    "    \n",
    "cv2.waitKey(0)   \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Hull\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/hand.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.waitKey(0) \n",
    "\n",
    "# Threshold the image\n",
    "ret, thresh = cv2.threshold(gray, 176, 255, 0)\n",
    "\n",
    "# Find contours \n",
    "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "# Sort Contors by area and then remove the largest frame contour\n",
    "n = len(contours) - 1\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=False)[:n]\n",
    "\n",
    "# Iterate through contours and draw the convex hull\n",
    "for c in contours:\n",
    "    hull = cv2.convexHull(c)\n",
    "    cv2.drawContours(image, [hull], 0, (0, 255, 0), 2)\n",
    "    cv2.imshow('Convex Hull', image)\n",
    "\n",
    "    # Iterate through each contour and compute the approx contour\n",
    "\n",
    "for c in contours:\n",
    "    # Calculate accuracy as a percent of the contour perimeter\n",
    "    accuracy = 0.001 * cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, accuracy, True)\n",
    "    cv2.drawContours(image, [approx], 0, (0, 255, 0), 2)\n",
    "    cv2.imshow('Approx Poly DP', image)\n",
    "\n",
    "cv2.waitKey(0)    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cv2.matchShapes(contour template, contour, method, method parameter)**\n",
    "\n",
    "**Output**  match value (lower values means a closer match)\n",
    "\n",
    "- Contour Template  This is our reference contour that were trying to find in the new image\n",
    "- Contour  The individual contour we are checking against\n",
    "- Method  Type of contour matching (1, 2, 3)\n",
    "- Method Parameter  leave alone as 0.0 (not fully utilized in python OpenCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13081816783853514\n",
      "0.1590200533978871\n",
      "0.1498791568252558\n",
      "0.07094034474475601\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the shape template or reference image\n",
    "template = cv2.imread('images/4star.jpg',0)\n",
    "cv2.imshow('Template', template)\n",
    "cv2.waitKey()\n",
    "\n",
    "# Load the target image with the shapes we're trying to match\n",
    "target = cv2.imread('images/shapestomatch.jpg')\n",
    "target_gray = cv2.cvtColor(target,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Threshold both images first before using cv2.findContours\n",
    "ret, thresh1 = cv2.threshold(template, 127, 255, 0)\n",
    "ret, thresh2 = cv2.threshold(target_gray, 127, 255, 0)\n",
    "\n",
    "# Find contours in template\n",
    "contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# We need to sort the contours by area so that we can remove the largest\n",
    "# contour which is the image outline\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# We extract the second largest contour which will be our template contour\n",
    "template_contour = contours[1]\n",
    "\n",
    "# Extract contours from second target image\n",
    "contours, hierarchy = cv2.findContours(thresh2, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for c in contours:\n",
    "    # Iterate through each contour in the target image and \n",
    "    # use cv2.matchShapes to compare contour shapes\n",
    "    match = cv2.matchShapes(template_contour, c, 3, 0.0)\n",
    "    print(match)\n",
    "    # If the match value is less than 0.15 we\n",
    "    if match < 0.15:\n",
    "        closest_contour = c\n",
    "    else:\n",
    "        closest_contour = [] \n",
    "                \n",
    "cv2.drawContours(target, [closest_contour], -1, (0,255,0), 3)\n",
    "cv2.imshow('Output', target)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
